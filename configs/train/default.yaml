model:
  name: "google/flan-t5-large"
  batch_size: 1
  grad_accum: 16
  lr: 5e-5
  epochs: 3
  warmup_steps: 100
  weight_decay: 0.01

training:
  output_dir: "checkpoints/${data.dataset}_fid"
  eval_steps: 500
  save_steps: 1000
  logging_steps: 50
  mixed_precision: "fp16"
